#include <stdio.h>        // Standard i/o
#include <stdlib.h>       //Standard library (for malloc, free, srand, rand)
#include <chrono>         // For timing functions
#include <cuda_runtime.h> //CUDA runtime API

#define N 1000000
#define threadsPerBlock 256 // Block dimension

void init_vector(float *v, int n)
{
    for (int i = 0; i < n; i++)
    {
        v[i] = (float)rand() / RAND_MAX;
    }
}

void vector_add_cpu(float *a, float *b, float *c, int n)
{
    for (int i = 0; i < n; i++)
    {
        c[i] = a[i] + b[i];
    }
}

__global__ void vector_add_gpu(float *a, float *b, float *c, int n)
{
    int i = threadIdx.x + blockIdx.x * blockDim.x;
    if (i < n)
    {
        c[i] = a[i] + b[i];
    }
}

int main()
{
    float *h_a, *h_b, *h_c_cpu;
    float *d_a, *d_b, *d_c;
    size_t size = N * sizeof(float);

    // Allocate host memory
    h_a = (float *)malloc(size);
    h_b = (float *)malloc(size);
    h_c_cpu = (float *)malloc(size);

    // Initialize vectors
    srand(time(NULL)); // Ensures that the random numbers generated by rand are different each time the program is run
    init_vector(h_a, N);
    init_vector(h_b, N);

    // Allocate device memory
    cudaMalloc(&d_a, size);
    cudaMalloc(&d_b, size);
    cudaMalloc(&d_c, size);

    // Copy data from host to device
    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);

    // Define grid dimension
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock; // Adding (threadsPerBlock-1) provides a ceiling effect

    // Warm-up runs
    printf("Performing warm-up runs \n");
    for (int i = 0; i < 3; i++)
    {
        vector_add_cpu(h_a, h_b, h_c_cpu, N);
        vector_add_gpu<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, N);
        cudaDeviceSynchronize();
    }

    // Benchmark CPU implementation
    printf("Performing CPU implementation \n");
    auto cpu_total_time = 0.0;
    for (int i = 0; i < 20; i++)
    {
        const auto start_time = std::chrono::system_clock::now();
        vector_add_cpu(h_a, h_b, h_c_cpu, N);
        const auto end_time = std::chrono::system_clock::now();
        // std::chrono::duration<double> elapsed = end_time - start_time; // in seconds
        std::chrono::microseconds elapsed_ms = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time);
        cpu_total_time += elapsed_ms.count();
    }
    double cpu_avg_time = cpu_total_time / 20;

    // Benchmark GPU implementation
    printf("Performing GPU implementation \n");
    double gpu_total_time = 0.0;
    for (int i = 0; i < 20; i++)
    {
        const auto start_time = std::chrono::system_clock::now();
        vector_add_gpu<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, N);
        cudaDeviceSynchronize();
        const auto end_time = std::chrono::system_clock::now();
        std::chrono::microseconds elapsed_ms = std::chrono::duration_cast<std::chrono::microseconds>(end_time - start_time);
        gpu_total_time += elapsed_ms.count();
    }
    double gpu_avg_time = gpu_total_time / 20;

    printf("CPU average time: %f microseconds \n", cpu_avg_time);
    printf("GPU average time: %f microseconds \n", gpu_avg_time);
    printf("Speedup: %fx\n", cpu_avg_time / gpu_avg_time);

    // Free memory
    free(h_a);
    free(h_b);
    free(h_c_cpu);
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    return 0;
}
